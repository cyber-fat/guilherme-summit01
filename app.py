import streamlit as st
import os
import time

# Imports do LangChain
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# ============================
# CONFIGURA√á√ÉO DA P√ÅGINA
# ============================
st.set_page_config(page_title="Summit IA Assistant", page_icon="üéì")

st.title("üéì Assistente Summit IA na Educa√ß√£o")
st.markdown("Pergunte sobre palestrantes, temas e conte√∫dos do evento.")

# Bot√£o na barra lateral para limpar mem√≥ria
with st.sidebar:
    st.header("Controles")
    if st.button("üîÑ Atualizar C√©rebro (Limpar Cache)"):
        st.cache_resource.clear()
        st.success("Mem√≥ria limpa! Recarregando...")
        time.sleep(1)
        st.rerun()

# ============================
# CACHING (Carregamento do Banco)
# ============================
@st.cache_resource
def carregar_banco():
    # Verifica se a pasta existe
    if not os.path.exists("vector_store"):
        st.error("‚ùå Erro: A pasta 'vector_store' n√£o foi encontrada. Verifique se voc√™ fez o upload dos arquivos.")
        return None
    
    try:
        embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/paraphrase-multilingual-mpnet-base-v2")
        # allow_dangerous_deserialization √© necess√°rio para FAISS local confi√°vel
        vectorstore = FAISS.load_local("vector_store", embeddings, allow_dangerous_deserialization=True)
        return vectorstore
    except Exception as e:
        st.error(f"Erro ao carregar o banco de vetores: {e}")
        return None

def gerar_variacoes_pergunta(llm, pergunta_original):
    """Gera m√∫ltiplas vers√µes da pergunta para melhorar a busca (Multi-Query Retrieval)"""
    template = """
    Voc√™ √© um assistente de busca. O usu√°rio est√° perguntando sobre o "Summit Explore a IA na Educa√ß√£o".
    Gere 4 vers√µes diferentes da pergunta do usu√°rio para encontrar a resposta correta nos documentos.
    
    Diretrizes:
    1. Se disser "evento", substitua por "Summit Explore a IA".
    2. Se perguntar de "palestrantes", inclua: "Lista oficial de nomes", "Rela√ß√£o completa de convidados".
    
    Pergunta original: {question}
    Retorne apenas as 4 perguntas, uma por linha. Sem numera√ß√£o.
    """
    prompt = ChatPromptTemplate.from_template(template)
    chain = prompt | llm | StrOutputParser()
    resultado = chain.invoke({"question": pergunta_original})
    return [p.strip() for p in resultado.split('\n') if p.strip()]

# ============================
# L√ìGICA PRINCIPAL
# ============================

# 1. Configura API Key
# Tenta pegar dos segredos do Streamlit ou vari√°vel de ambiente local
if "OPENAI_API_KEY" in st.secrets:
    os.environ["OPENAI_API_KEY"] = st.secrets["OPENAI_API_KEY"]

if not os.environ.get("OPENAI_API_KEY"):
    st.warning("‚ö†Ô∏è Chave de API n√£o configurada. Configure o .streamlit/secrets.toml ou o arquivo .env")
    st.stop()

# 2. Carrega o Banco
vectorstore = carregar_banco()
if not vectorstore:
    st.stop()

# 3. Inicializa Hist√≥rico de Chat
if "messages" not in st.session_state:
    st.session_state["messages"] = [{"role": "assistant", "content": "Ol√°! Sou a IA do Summit. Pode me perguntar sobre palestrantes, temas ou hor√°rios."}]

# 4. Exibe mensagens anteriores na tela (Re-renderiza o hist√≥rico)
for msg in st.session_state.messages:
    st.chat_message(msg["role"]).write(msg["content"])

# 5. Caixa de Entrada do Usu√°rio
if prompt := st.chat_input("Digite sua pergunta..."):
    # Adiciona msg do usu√°rio ao hist√≥rico visual e ao estado
    st.session_state.messages.append({"role": "user", "content": prompt})
    st.chat_message("user").write(prompt)

    # L√≥gica de Resposta
    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        message_placeholder.markdown("üîé *Pesquisando nos documentos...*")
        
        try:
            # Inicializa LLM
            llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0)
            
            # A. Gera Varia√ß√µes (Multi-Query Manual)
            variacoes = gerar_variacoes_pergunta(llm, prompt)
            
            # B. Busca Robusta Otimizada
            docs_encontrados = []
            
            # ALTERA√á√ÉO IMPORTANTE: Reduzido k de 25 para 7 para evitar polui√ß√£o de contexto
            # 4 varia√ß√µes x 7 docs = ~28 docs totais (gerenci√°vel)
            for p in variacoes:
                docs = vectorstore.similarity_search(p, k=7)
                docs_encontrados.extend(docs)
            
            # Deduplica√ß√£o baseada no conte√∫do exato
            docs_unicos = {d.page_content: d for d in docs_encontrados}
            lista_final = list(docs_unicos.values())
            
            # Monta o contexto final
            contexto_texto = "\n\n".join([f"FONTE: {d.metadata.get('source', 'Desconhecida')}\nCONTE√öDO: {d.page_content}" for d in lista_final])

            # FERRAMENTA DE DEBUG (Vis√≠vel apenas se clicar)
            # Isso ajuda a ver se o RAG est√° trazendo lixo ou repetindo texto
            with st.expander("üõ†Ô∏è Ver Contexto Recuperado (Debug)"):
                st.write(f"Varia√ß√µes geradas: {variacoes}")
                st.write(f"Total de documentos √∫nicos recuperados: {len(lista_final)}")
                st.text_area("Conte√∫do Bruto enviado para a IA:", contexto_texto, height=200)

            # C. Gera Resposta Final
            template_resposta = """
            Voc√™ √© um assistente especialista no Summit 'Explore a IA na Educa√ß√£o'.
            
            GLOSS√ÅRIO:
            - "Evento", "Confer√™ncia" = "Summit Explore a IA na Educa√ß√£o".
            - "Palestrantes" = Use a LISTA MESTRA prioritariamente.
            
            DIRETRIZES DE RESPOSTA (R√çGIDAS):
            1. Use APENAS as informa√ß√µes fornecidas no CONTEXTO abaixo.
            2. Se perguntarem sobre PALESTRANTES: 
               - Liste TODOS os nomes √∫nicos encontrados.
               - DEDUPLIQUE: Se o nome "Ana" aparece 3 vezes no texto, escreva apenas uma vez.
               - Organize em ordem alfab√©tica.
               - N√£o invente nomes que n√£o est√£o no texto.
            3. Se a informa√ß√£o n√£o estiver no contexto, diga: "N√£o encontrei essa informa√ß√£o nos documentos oficiais."
            
            CONTEXTO DOS DOCUMENTOS:
            {context}

            PERGUNTA DO USU√ÅRIO: {question}
            """
            
            chain = ChatPromptTemplate.from_template(template_resposta) | llm | StrOutputParser()
            
            # Executa a chain
            resposta_final = chain.invoke({"context": contexto_texto, "question": prompt})
            
            # Exibe resposta final
            message_placeholder.markdown(resposta_final)
            
            # Salva no hist√≥rico para manter a conversa
            st.session_state.messages.append({"role": "assistant", "content": resposta_final})
            
        except Exception as e:
            st.error(f"Ocorreu um erro ao processar sua pergunta: {e}")
