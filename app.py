import streamlit as st
import os
import time

# Imports do LangChain
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser


# Bot√£o na barra lateral para limpar mem√≥ria
with st.sidebar:
    if st.button("üîÑ Atualizar C√©rebro (Limpar Cache)"):
        st.cache_resource.clear()
        st.success("Mem√≥ria limpa! Recarregando...")
        time.sleep(1)
        st.rerun()


# ============================
# CONFIGURA√á√ÉO DA P√ÅGINA
# ============================
st.set_page_config(page_title="Summit IA Assistant", page_icon="üéì")

st.title("üéì Assistente Summit IA na Educa√ß√£o")
st.markdown("Pergunte sobre palestrantes, temas e conte√∫dos do evento.")

# ============================
# CACHING (Para n√£o recarregar o banco a cada clique)
# ============================
@st.cache_resource
def carregar_banco():
    # Verifica se a pasta existe
    if not os.path.exists("vector_store"):
        st.error("‚ùå Erro: A pasta 'vector_store' n√£o foi encontrada no reposit√≥rio.")
        return None
    
    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/paraphrase-multilingual-mpnet-base-v2")
    # allow_dangerous_deserialization √© necess√°rio para FAISS local
    vectorstore = FAISS.load_local("vector_store", embeddings, allow_dangerous_deserialization=True)
    return vectorstore

def gerar_variacoes_pergunta(llm, pergunta_original):
    template = """
    Voc√™ √© um assistente de busca. O usu√°rio est√° perguntando sobre o "Summit Explore a IA na Educa√ß√£o".
    Gere 4 vers√µes diferentes da pergunta do usu√°rio para encontrar a resposta correta.
    
    Diretrizes:
    1. Se disser "evento", substitua por "Summit Explore a IA".
    2. Se perguntar de "palestrantes", inclua: "Lista oficial de nomes", "Rela√ß√£o completa de convidados".
    
    Pergunta original: {question}
    Retorne apenas as 4 perguntas, uma por linha.
    """
    prompt = ChatPromptTemplate.from_template(template)
    chain = prompt | llm | StrOutputParser()
    resultado = chain.invoke({"question": pergunta_original})
    return [p.strip() for p in resultado.split('\n') if p.strip()]

# ============================
# L√ìGICA PRINCIPAL
# ============================

# 1. Configura API Key (Pega dos Segredos do Streamlit Cloud)
if "OPENAI_API_KEY" in st.secrets:
    os.environ["OPENAI_API_KEY"] = st.secrets["OPENAI_API_KEY"]
else:
    # Fallback para rodar local se tiver no .env ou vari√°vel de sistema
    pass

if not os.environ.get("OPENAI_API_KEY"):
    st.warning("‚ö†Ô∏è Chave de API n√£o configurada. O Chat n√£o funcionar√°.")
    st.stop()

# 2. Carrega o Banco
vectorstore = carregar_banco()
if not vectorstore:
    st.stop()

# 3. Inicializa Hist√≥rico de Chat
if "messages" not in st.session_state:
    st.session_state["messages"] = [{"role": "assistant", "content": "Ol√°! Sou a IA do Summit. Pode me perguntar sobre palestrantes, temas ou hor√°rios."}]

# 4. Exibe mensagens anteriores na tela
for msg in st.session_state.messages:
    st.chat_message(msg["role"]).write(msg["content"])

# 5. Caixa de Entrada do Usu√°rio
if prompt := st.chat_input("Digite sua pergunta..."):
    # Adiciona msg do usu√°rio ao hist√≥rico
    st.session_state.messages.append({"role": "user", "content": prompt})
    st.chat_message("user").write(prompt)

    # L√≥gica de Resposta
    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        message_placeholder.markdown("üîé *Pesquisando nos documentos...*")
        
        try:
            llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0)
            
            # A. Gera Varia√ß√µes (Multi-Query Manual)
            variacoes = gerar_variacoes_pergunta(llm, prompt)
            
            # B. Busca Robusta (High Recall)
            docs_encontrados = []
            for p in variacoes:
                docs = vectorstore.similarity_search(p, k=10)
                docs_encontrados.extend(docs)
            
            # Deduplica√ß√£o
            docs_unicos = {d.page_content: d for d in docs_encontrados}
            lista_final = list(docs_unicos.values())
            
            contexto_texto = "\n\n".join([f"FONTE: {d.metadata.get('source')}\nCONTE√öDO: {d.page_content}" for d in lista_final])

            # C. Gera Resposta Final
            template_resposta = """
            Voc√™ √© um assistente especialista no Summit 'Explore a IA na Educa√ß√£o'.
            
            GLOSS√ÅRIO:
            - "Evento" = "Summit Explore a IA na Educa√ß√£o".
            - "Palestrantes" = Use a LISTA MESTRA prioritariamente.
            
            INSTRU√á√ïES:
            Use o contexto abaixo. Se houver listas divididas, junte-as.
            Se n√£o souber, diga que n√£o sabe.

            CONTEXTO:
            {context}

            PERGUNTA: {question}
            """
            
            chain = ChatPromptTemplate.from_template(template_resposta) | llm | StrOutputParser()
            
            resposta_final = chain.invoke({"context": contexto_texto, "question": prompt})
            
            # Exibe resposta final
            message_placeholder.markdown(resposta_final)
            
            # Salva no hist√≥rico
            st.session_state.messages.append({"role": "assistant", "content": resposta_final})
            
        except Exception as e:
            st.error(f"Erro: {e}")
