import streamlit as st
import os
import time

# Imports do LangChain
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# ============================
# CONFIGURA√á√ÉO DA P√ÅGINA
# ============================
st.set_page_config(page_title="Summit IA Assistant", page_icon="üéì")

st.title("üéì Assistente Summit IA na Educa√ß√£o")
st.markdown("Pergunte sobre palestrantes, temas e conte√∫dos do evento.")

# Bot√£o na barra lateral para limpar mem√≥ria
with st.sidebar:
    st.header("Controles")
    if st.button("üîÑ Atualizar C√©rebro (Limpar Cache)"):
        st.cache_resource.clear()
        st.session_state.messages = [] # Limpa tamb√©m o chat visual
        st.success("Mem√≥ria limpa! Recarregando...")
        time.sleep(1)
        st.rerun()

# ============================
# CACHING (Carregamento do Banco)
# ============================
@st.cache_resource
def carregar_banco():
    if not os.path.exists("vector_store"):
        st.error("‚ùå Erro: A pasta 'vector_store' n√£o foi encontrada. Verifique se voc√™ fez o upload dos arquivos.")
        return None
    
    try:
        embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/paraphrase-multilingual-mpnet-base-v2")
        # allow_dangerous_deserialization √© necess√°rio para FAISS local confi√°vel
        vectorstore = FAISS.load_local("vector_store", embeddings, allow_dangerous_deserialization=True)
        return vectorstore
    except Exception as e:
        st.error(f"Erro ao carregar o banco de vetores: {e}")
        return None

def gerar_variacoes_pergunta(llm, pergunta_original):
    """Gera m√∫ltiplas vers√µes da pergunta para melhorar a busca (Multi-Query Retrieval)"""
    template = """
    Voc√™ √© um assistente de busca. O usu√°rio est√° perguntando sobre o "Summit Explore a IA na Educa√ß√£o".
    Gere 4 vers√µes diferentes da pergunta do usu√°rio para encontrar a resposta correta nos documentos.
    
    Diretrizes:
    1. Se disser "evento", substitua por "Summit Explore a IA".
    2. Se perguntar de "palestrantes", inclua: "Lista oficial de nomes".
    
    Pergunta original: {question}
    Retorne apenas as 4 perguntas, uma por linha. Sem numera√ß√£o.
    """
    prompt = ChatPromptTemplate.from_template(template)
    chain = prompt | llm | StrOutputParser()
    resultado = chain.invoke({"question": pergunta_original})
    return [p.strip() for p in resultado.split('\n') if p.strip()]

def formatar_historico(messages):
    """Transforma o hist√≥rico de chat do Streamlit em texto para a IA"""
    # Pega as √∫ltimas 6 mensagens para dar contexto sem estourar tokens
    # Ignora a primeira mensagem se for apenas a sauda√ß√£o do sistema
    historico_recente = messages[-6:-1] 
    texto_historico = ""
    for msg in historico_recente:
        role = "Usu√°rio" if msg["role"] == "user" else "Assistente"
        texto_historico += f"{role}: {msg['content']}\n"
    return texto_historico if texto_historico else "Nenhum hist√≥rico anterior."

# ============================
# L√ìGICA PRINCIPAL
# ============================

# 1. Configura API Key
if "OPENAI_API_KEY" in st.secrets:
    os.environ["OPENAI_API_KEY"] = st.secrets["OPENAI_API_KEY"]

if not os.environ.get("OPENAI_API_KEY"):
    st.warning("‚ö†Ô∏è Chave de API n√£o configurada. Configure o .streamlit/secrets.toml")
    st.stop()

# 2. Carrega o Banco
vectorstore = carregar_banco()
if not vectorstore:
    st.stop()

# 3. Inicializa Hist√≥rico de Chat
if "messages" not in st.session_state:
    st.session_state["messages"] = [{"role": "assistant", "content": "Ol√°! Sou a IA do Summit. Pode me perguntar sobre palestrantes, temas ou hor√°rios."}]

# 4. Exibe mensagens anteriores na tela
for msg in st.session_state.messages:
    st.chat_message(msg["role"]).write(msg["content"])

# 5. Caixa de Entrada do Usu√°rio
if prompt := st.chat_input("Digite sua pergunta..."):
    # Adiciona msg do usu√°rio ao hist√≥rico visual
    st.session_state.messages.append({"role": "user", "content": prompt})
    st.chat_message("user").write(prompt)

    # L√≥gica de Resposta
    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        message_placeholder.markdown("üîé *Pesquisando...*")
        
        try:
            llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0)
            
            # A. Prepara√ß√£o do Contexto
            # 1. Recupera hist√≥rico para entender refer√™ncias ("ele", "ela", "o evento")
            historico_str = formatar_historico(st.session_state.messages)

            # 2. Gera Varia√ß√µes para busca
            # Combinamos a pergunta atual com um pingo de contexto se necess√°rio
            variacoes = gerar_variacoes_pergunta(llm, prompt)
            
            # B. Busca Robusta Otimizada (k=7)
            docs_encontrados = []
            for p in variacoes:
                docs = vectorstore.similarity_search(p, k=7)
                docs_encontrados.extend(docs)
            
            # Deduplica√ß√£o
            docs_unicos = {d.page_content: d for d in docs_encontrados}
            lista_final = list(docs_unicos.values())
            
            contexto_texto = "\n\n".join([f"FONTE: {d.metadata.get('source', 'Desconhecida')}\nCONTE√öDO: {d.page_content}" for d in lista_final])

            # Debug (Opcional - vis√≠vel apenas se expandir)
            with st.expander("üõ†Ô∏è Ver Contexto e Mem√≥ria (Debug)"):
                st.write("**Hist√≥rico enviado:**")
                st.text(historico_str)
                st.write(f"**Documentos recuperados:** {len(lista_final)}")

            # C. Gera Resposta Final com Mem√≥ria
            template_resposta = """
            Voc√™ √© um assistente especialista no Summit 'Explore a IA na Educa√ß√£o'.
            
            HIST√ìRICO DA CONVERSA:
            {history}
            
            GLOSS√ÅRIO:
            - "Evento" = "Summit Explore a IA na Educa√ß√£o".
            
            DIRETRIZES DE RESPOSTA:
            1. Use o CONTEXTO abaixo para responder √† PERGUNTA ATUAL.
            2. Se a pergunta usar pronomes como "ele", "ela", "disso", use o HIST√ìRICO para entender a quem se refere.
            3. Se perguntarem sobre PALESTRANTES: 
               - Liste nomes √∫nicos. N√ÉO repita nomes.
               - Se a lista for longa, cite os principais ou pe√ßa para especificar.
            4. Se a informa√ß√£o n√£o estiver no contexto, diga que n√£o sabe.
            
            CONTEXTO DOS DOCUMENTOS:
            {context}

            PERGUNTA ATUAL: {question}
            """
            
            chain = ChatPromptTemplate.from_template(template_resposta) | llm | StrOutputParser()
            
            # Passamos prompt, contexto E hist√≥rico
            resposta_final = chain.invoke({
                "context": contexto_texto, 
                "question": prompt,
                "history": historico_str
            })
            
            # Exibe resposta final
            message_placeholder.markdown(resposta_final)
            
            # Salva no hist√≥rico
            st.session_state.messages.append({"role": "assistant", "content": resposta_final})
            
        except Exception as e:
            st.error(f"Erro ao processar: {e}")
